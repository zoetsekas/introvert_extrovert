{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91718,"databundleVersionId":12738969,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/zoetsekas/ensemble-of-xgboost-lightgbm-and-pytorch?scriptVersionId=252451326\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"6b05e6c9-4af4-4487-9f1d-aeb533f4e7b4","_cell_guid":"629ab133-70d8-4bd5-b7df-2c4093fd5513","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-24T18:39:42.221766Z","iopub.execute_input":"2025-07-24T18:39:42.222029Z","iopub.status.idle":"2025-07-24T18:39:42.228113Z","shell.execute_reply.started":"2025-07-24T18:39:42.222012Z","shell.execute_reply":"2025-07-24T18:39:42.22743Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay, auc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nimport lightgbm as lgb\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.utils.tensorboard import SummaryWriter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport ray\nfrom ray import tune, air\nfrom ray.tune import TuneConfig\nfrom ray.tune.tuner import Tuner\nfrom ray.tune.search.optuna import OptunaSearch\nfrom optuna.samplers import TPESampler\nfrom ray.tune.logger import TBXLoggerCallback\nimport os\nfrom itertools import combinations\nfrom tqdm.auto import tqdm\n\n# --- Environment Variable to Suppress Warnings ---\nos.environ['RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS'] = '0'\n\n\ndef check_gpu_with_pytorch():\n    \"\"\"Checks if PyTorch can access and use the GPU, printing the status.\"\"\"\n    print(\"Performing GPU check for PyTorch...\")\n    if torch.cuda.is_available():\n        print(f\"✅ GPU check successful. PyTorch can access GPU: {torch.cuda.get_device_name(0)}\")\n        return True\n    else:\n        print(\"❌ GPU check FAILED. PyTorch could not find a CUDA-enabled GPU.\")\n        return False\n\ndef check_gpu_with_xgboost():\n    \"\"\"Checks if XGBoost can access and use the GPU, printing the status.\"\"\"\n    print(\"Performing GPU check for XGBoost...\")\n    try:\n        xgb.train({'device': 'cuda'}, xgb.DMatrix(np.random.rand(10, 2), label=np.random.randint(0, 2, 10)), 1)\n        print(\"✅ GPU check successful. XGBoost can access the GPU.\")\n        return True\n    except xgb.core.XGBoostError:\n        print(\"❌ GPU check FAILED. XGBoost could not utilize the GPU.\")\n        return False\n\n# --- PyTorch Neural Network Definition ---\nclass Net(nn.Module):\n    \"\"\"A dynamically configurable multi-layer perceptron for binary classification.\"\"\"\n    def __init__(self, input_dim, n_layers, n_nodes, dropout_rate, activation_fn):\n        \"\"\"\n        Initializes the neural network.\n\n        Args:\n            input_dim (int): The number of input features.\n            n_layers (int): The number of hidden layers.\n            n_nodes (int): The number of nodes in each hidden layer.\n            dropout_rate (float): The dropout rate to apply after each hidden layer.\n            activation_fn (str): The name of the torch.nn activation function to use.\n        \"\"\"\n        super(Net, self).__init__()\n        layers = []\n        act_fn = getattr(nn, activation_fn)()\n        layers.append(nn.Linear(input_dim, n_nodes))\n        layers.append(act_fn)\n        layers.append(nn.Dropout(dropout_rate))\n        for _ in range(n_layers - 1):\n            layers.append(nn.Linear(n_nodes, n_nodes))\n            layers.append(act_fn)\n            layers.append(nn.Dropout(dropout_rate))\n        layers.append(nn.Linear(n_nodes, 1))\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"Defines the forward pass of the neural network.\"\"\"\n        return self.network(x)\n\n# --- Data Processing and Feature Engineering Functions ---\ndef perform_eda(df, stage=\"Initial\"):\n    \"\"\"\n    Performs and prints basic Exploratory Data Analysis on the dataframe.\n\n    Args:\n        df (pd.DataFrame): The dataframe to analyze.\n        stage (str): A string to identify the stage of EDA (e.g., \"Initial\", \"Cleaned\").\n    \"\"\"\n    print(f\"\\n--- {stage} Exploratory Data Analysis ---\")\n    print(\"\\nDataset Info:\")\n    df.info()\n    print(\"\\nDescriptive Statistics:\")\n    print(df.describe())\n    print(\"\\nPersonality Distribution:\")\n    print(df['Personality'].value_counts(normalize=True))\n    \n    # Visualize the correlation matrix of the original features\n    print(\"\\nCorrelation Matrix:\")\n    eda_df = df.copy()\n    for col in eda_df.columns:\n        if eda_df[col].dtype == 'object':\n            if eda_df[col].isnull().any():\n                eda_df[col] = eda_df[col].fillna(eda_df[col].mode()[0])\n            eda_df[col] = LabelEncoder().fit_transform(eda_df[col])\n    corr_matrix = eda_df.drop('id', axis=1).corr()\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n    plt.title(f'{stage} Correlation Matrix of Features')\n    plt.savefig(f'{stage.lower()}_correlation_matrix.png')\n    print(f\"{stage} correlation matrix plot saved to {stage.lower()}_correlation_matrix.png\")\n    plt.show()\n\ndef analyze_and_clean_with_baseline(df):\n    \"\"\"\n    Trains a baseline model to find and remove potentially mislabeled data.\n\n    Args:\n        df (pd.DataFrame): The training dataframe to clean.\n\n    Returns:\n        pd.DataFrame: The cleaned dataframe with suspicious labels removed.\n    \"\"\"\n    print(\"\\n--- Baseline Model Analysis and Cleaning ---\")\n    baseline_df = preprocess_data(df.copy())\n    X = baseline_df.drop(['id', 'Personality'], axis=1)\n    y = LabelEncoder().fit_transform(baseline_df['Personality'])\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    baseline_model = LogisticRegression(random_state=42)\n    baseline_model.fit(X_scaled, y)\n    y_hat = baseline_model.predict_proba(X_scaled)[:, 1]\n\n    # Identify samples where the model is confident in a prediction that contradicts the label\n    exclude_these = ((y == 1) & (y_hat < 0.25)) | ((y == 0) & (y_hat > 0.85))\n    exclude_indices = df.index[exclude_these]\n    \n    print(f\"Identified {len(exclude_indices)} samples to remove based on baseline model confidence.\")\n    \n    cleaned_df = df.drop(exclude_indices).reset_index(drop=True)\n    print(f\"Original shape: {df.shape}, Cleaned shape: {cleaned_df.shape}\")\n    \n    # --- New Confusion Dots and ROC Curve Plot ---\n    y_thresh = 0.5\n    ysframe = pd.DataFrame([y, y_hat], index=['y', 'y_prob']).transpose()\n    ysframe['y (blurred)'] = ysframe['y'] + 0.1 * np.random.randn(len(ysframe))\n\n    # Plot the real y (blurred) vs the predicted probability\n    ysframe.plot.scatter('y_prob', 'y (blurred)', figsize=(10, 4),\n                         s=2, xlim=(0.0, 1.0), ylim=(1.8, -0.8), alpha=0.35)\n    plt.plot([0.0, y_thresh], [0.0, 0.0], '-', color='green', linewidth=3)\n    plt.plot([y_thresh, y_thresh], [0.0, 1.0], '-', color='gray', linewidth=2)\n    plt.plot([y_thresh, 1.0], [1.0, 1.0], '-', color='green', linewidth=3)\n    plt.title(\"Confusion-dots Plot: Baseline Model\", fontsize=16)\n    ythr2 = y_thresh / 2.0\n    plt.text(ythr2 - 0.03, 1.52, \"FN\", fontsize=16, color='red')\n    plt.text(ythr2 + 0.5 - 0.03, 1.52, \"TP\", fontsize=16, color='green')\n    plt.text(ythr2 - 0.03, -0.50, \"TN\", fontsize=16, color='green')\n    plt.text(ythr2 + 0.5 - 0.03, -0.50, \"FP\", fontsize=16, color='red')\n    plt.savefig(\"baseline_confusion_dots.png\")\n    print(\"Baseline confusion dots plot saved to baseline_confusion_dots.png\")\n    plt.show()\n\n    # Make the ROC plot\n    fpr, tpr, _ = roc_curve(y, y_hat)\n    roc_auc = auc(fpr, tpr)\n    plt.figure(figsize=(5, 5)) # Create a new figure for the ROC curve\n    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Baseline Model')\n    display.plot()\n    plt.plot([0., 1.], [0., 1.], c='gray', alpha=0.3)\n    plt.title(\"Baseline Model ROC Curve\")\n    plt.savefig(\"baseline_roc_curve.png\")\n    print(\"Baseline ROC curve plot saved to baseline_roc_curve.png\")\n    plt.show()\n    \n    return cleaned_df\n\ndef drop_duplicates(df):\n    \"\"\"Checks for and removes duplicate rows from the dataframe.\"\"\"\n    print(\"\\n--- Dropping Duplicates ---\")\n    print(f\"Original shape: {df.shape}\")\n    df_cleaned = df.drop_duplicates()\n    print(f\"Shape after dropping duplicates: {df_cleaned.shape}\")\n    print(f\"Removed {df.shape[0] - df_cleaned.shape[0]} duplicate rows.\")\n    return df_cleaned\n\ndef plot_individual_features(df):\n    \"\"\"Creates and saves a pairplot of numerical features, colored by Personality.\"\"\"\n    print(\"\\n--- Creating Pairplot of Numerical Features ---\")\n    numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n    if 'id' in numerical_features:\n        numerical_features.remove('id')\n    \n    # Create a dataframe for the pairplot\n    pairplot_df = df[numerical_features + ['Personality']]\n    \n    sns.pairplot(pairplot_df, hue='Personality', diag_kind='kde')\n    plt.suptitle('Pairplot of Numerical Features by Personality', y=1.02)\n    plt.savefig('eda_pairplot.png')\n    print(\"Pairplot saved to eda_pairplot.png\")\n    plt.show()\n\n\ndef select_features_by_correlation(df, threshold=0.9):\n    \"\"\"Identifies and returns a list of highly correlated features to drop.\"\"\"\n    corr_matrix = df.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n    print(f\"\\nFound {len(to_drop)} features to drop with correlation > {threshold}: {to_drop}\")\n    return to_drop\n\ndef select_k_best_features(X, y, X_test, k=20):\n    \"\"\"Selects the top k features using SelectKBest.\"\"\"\n    print(f\"\\n--- Selecting Top {k} Features using SelectKBest ---\")\n    selector = SelectKBest(score_func=f_classif, k=k)\n    X_new = selector.fit_transform(X, y)\n    \n    # Get the columns to keep\n    cols_to_keep = selector.get_support(indices=True)\n    X_selected = X.iloc[:, cols_to_keep]\n    X_test_selected = X_test.iloc[:, cols_to_keep]\n    \n    print(f\"Selected {len(X_selected.columns)} features: {X_selected.columns.tolist()}\")\n    return X_selected, X_test_selected\n\ndef preprocess_data(df):\n    \"\"\"Preprocesses the data by handling NaNs and encoding categorical features.\"\"\"\n    print(\"  > Preprocessing data (handling NaNs and encoding)...\")\n    df_copy = df.copy()\n    for col in ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']:\n        if df_copy[col].isnull().any():\n            df_copy[col] = df_copy[col].fillna(df_copy[col].mean())\n    for col in ['Stage_fear', 'Drained_after_socializing']:\n        if df_copy[col].isnull().any():\n            df_copy[col] = df_copy[col].fillna(df_copy[col].mode()[0])\n    for col in ['Stage_fear', 'Drained_after_socializing']:\n        if df_copy[col].dtype == 'object':\n            df_copy[col] = LabelEncoder().fit_transform(df_copy[col])\n    return df_copy\n\ndef feature_engineer(df):\n    \"\"\"Creates new interaction, ratio, and binned features.\"\"\"\n    print(\"  > Engineering new features (interactions, bins)...\")\n    df_copy = df.copy()\n    df_copy['Social_Interaction_Score'] = df_copy['Social_event_attendance'] * df_copy['Friends_circle_size']\n    df_copy['Alone_vs_Social_Ratio'] = df_copy['Time_spent_Alone'] / (df_copy['Social_event_attendance'] + 1)\n    \n    # Binning Features\n    df_copy['Time_Alone_Bin'] = pd.qcut(df_copy['Time_spent_Alone'], q=4, labels=False, duplicates='drop')\n    df_copy['Friends_Circle_Bin'] = pd.qcut(df_copy['Friends_circle_size'], q=4, labels=False, duplicates='drop')\n    df_copy['Post_Frequency_Bin'] = pd.qcut(df_copy['Post_frequency'], q=4, labels=False, duplicates='drop')\n    return df_copy\n\ndef add_cluster_features(train_df, test_df, features, n_clusters=4):\n    \"\"\"Adds a new feature based on K-Means clustering.\"\"\"\n    print(f\"\\nCreating user clusters with K-Means (k={n_clusters})...\")\n    scaler = StandardScaler()\n    train_scaled = scaler.fit_transform(train_df[features])\n    test_scaled = scaler.transform(test_df[features])\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    train_df['User_Cluster'] = kmeans.fit_predict(train_scaled)\n    test_df['User_Cluster'] = kmeans.predict(test_scaled)\n    return train_df, test_df\n\ndef add_groupby_features(train_df, test_df):\n    \"\"\"Creates groupby aggregate features.\"\"\"\n    print(\"\\nAdding groupby aggregate features...\")\n    \n    groupby_cols = ['Time_Alone_Bin', 'Friends_Circle_Bin', 'Post_Frequency_Bin', 'Stage_fear', 'Drained_after_socializing', 'User_Cluster']\n    agg_cols = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n    \n    for group_col in tqdm(groupby_cols, desc=\"Groupby Features\"):\n        agg_stats = train_df.groupby(group_col)[agg_cols].agg(['mean', 'std', 'min', 'max']).reset_index()\n        \n        # Create unique column names to avoid collisions\n        new_cols = [group_col]\n        for col in agg_stats.columns.levels[0]:\n            if col == group_col:\n                continue\n            for stat in agg_stats.columns.levels[1]:\n                if stat == '':\n                    continue\n                new_cols.append(f'{group_col}_{col}_{stat}')\n        agg_stats.columns = new_cols\n        \n        train_df = pd.merge(train_df, agg_stats, on=group_col, how='left')\n        test_df = pd.merge(test_df, agg_stats, on=group_col, how='left')\n        \n        # Fill NaNs in test set after the merge\n        for col in new_cols:\n            if col != group_col:\n                median_val = train_df[col].median()\n                test_df[col] = test_df[col].fillna(median_val)\n\n    return train_df, test_df\n\n# --- OOF Prediction Generation Functions ---\n\ndef generate_xgb_oof(config, X, y, X_test):\n    \"\"\"Generates out-of-fold predictions for XGBoost.\"\"\"\n    print(\"\\n--- Generating OOF predictions for XGBoost ---\")\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(X))\n    test_preds_list = []\n    scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n\n    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), total=5, desc=\"XGB OOF Folds\")):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train = y.iloc[train_idx]\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val)\n        dtest = xgb.DMatrix(X_test)\n        params = {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42 + fold, 'tree_method': 'hist', 'device': 'cuda', 'scale_pos_weight': scale_pos_weight, **config}\n        model = xgb.train(params, dtrain, 1000, evals=[(dtrain, 'train')], early_stopping_rounds=50, verbose_eval=False)\n        oof_preds[val_idx] = model.predict(dval, iteration_range=(0, model.best_iteration))\n        test_preds_list.append(model.predict(dtest, iteration_range=(0, model.best_iteration)))\n    return oof_preds, np.mean(test_preds_list, axis=0)\n\ndef lgb_accuracy(y_pred, y_true):\n    \"\"\"Custom accuracy metric for LightGBM.\"\"\"\n    labels = y_true.get_label()\n    return 'accuracy', accuracy_score(labels, np.round(y_pred)), True\n    \ndef generate_lgb_oof(config, X, y, X_test):\n    \"\"\"Generates out-of-fold predictions for LightGBM.\"\"\"\n    print(\"\\n--- Generating OOF predictions for LightGBM ---\")\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(X))\n    test_preds_list = []\n    scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n\n    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), total=5, desc=\"LGB OOF Folds\")):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n        params = {'objective': 'binary', 'metric': 'None', 'random_state': 42 + fold, 'device': 'gpu', 'verbose': -1, 'scale_pos_weight': scale_pos_weight, **config}\n        model = lgb.train(\n            params, \n            dtrain, \n            1000, \n            valid_sets=[dval], \n            valid_names=['eval'], \n            feval=lgb_accuracy, \n            callbacks=[lgb.early_stopping(50, verbose=False)]\n        )\n        oof_preds[val_idx] = model.predict(X_val, num_iteration=model.best_iteration)\n        test_preds_list.append(model.predict(X_test, num_iteration=model.best_iteration))\n    return oof_preds, np.mean(test_preds_list, axis=0)\n\ndef generate_nn_oof(config, X, y, X_test):\n    \"\"\"Generates out-of-fold predictions for the PyTorch Neural Network.\"\"\"\n    print(\"\\n--- Generating OOF predictions for PyTorch NN ---\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(X))\n    test_preds_list = []\n    scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n\n    for fold, (train_idx, val_idx) in enumerate(tqdm(skf.split(X, y), total=5, desc=\"NN OOF Folds\")):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train = y.iloc[train_idx]\n        \n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        X_test_scaled = scaler.transform(X_test)\n        \n        train_ds = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))\n        val_ds = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32))\n        test_ds = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32))\n        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"])\n        test_loader = DataLoader(test_ds, batch_size=config[\"batch_size\"])\n\n        model = Net(X.shape[1], config['n_layers'], config['n_nodes'], config['dropout_rate'], config['activation']).to(device)\n        pos_weight_tensor = torch.tensor([scale_pos_weight], device=device)\n        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n        optimizer_class = getattr(optim, config['optimizer'])\n        optimizer = optimizer_class(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n\n        for _ in range(25): # More epochs for OOF generation\n            model.train()\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n        \n        model.eval()\n        fold_oof_preds = []\n        with torch.no_grad():\n            for inputs in val_loader:\n                outputs = model(inputs[0].to(device))\n                fold_oof_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n        oof_preds[val_idx] = np.array(fold_oof_preds).flatten()\n\n        fold_test_preds = []\n        with torch.no_grad():\n            for inputs in test_loader:\n                outputs = model(inputs[0].to(device))\n                fold_test_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n        test_preds_list.append(np.array(fold_test_preds).flatten())\n        \n    return oof_preds, np.mean(test_preds_list, axis=0)\n\n# --- Ray Tune Training Functions (unchanged, used for tuning only) ---\ndef train_xgb_cv(config, data):\n    \"\"\"Ray Tune trainable function for XGBoost CV.\"\"\"\n    X, y, scale_pos_weight = data\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    accuracies = []\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        params = {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'tree_method': 'hist', 'device': 'cuda', 'scale_pos_weight': scale_pos_weight, **config}\n        model = xgb.train(params, dtrain, 1000, evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n        preds = model.predict(dval, iteration_range=(0, model.best_iteration))\n        accuracies.append(accuracy_score(y_val, np.round(preds)))\n    tune.report({\"accuracy\": np.mean(accuracies)})\n\ndef train_lgb_cv(config, data):\n    \"\"\"Ray Tune trainable function for LightGBM CV.\"\"\"\n    X, y, scale_pos_weight = data\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    accuracies = []\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n        params = {'objective': 'binary', 'metric': 'None', 'device': 'gpu', 'verbose': -1, 'scale_pos_weight': scale_pos_weight, **config}\n        model = lgb.train(params, dtrain, 1000, valid_sets=[dval], valid_names=['eval'], feval=lgb_accuracy, callbacks=[lgb.early_stopping(50, verbose=False)])\n        preds = model.predict(X_val, num_iteration=model.best_iteration)\n        accuracies.append(accuracy_score(y_val, np.round(preds)))\n    tune.report({\"accuracy\": np.mean(accuracies)})\n\ndef train_nn_cv(config, data):\n    \"\"\"Ray Tune trainable function for PyTorch NN CV.\"\"\"\n    X, y, scale_pos_weight = data\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    accuracies = []\n    for train_idx, val_idx in skf.split(X, y):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        train_ds = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))\n        val_ds = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32), torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1))\n        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"])\n        model = Net(X.shape[1], config['n_layers'], config['n_nodes'], config['dropout_rate'], config['activation']).to(device)\n        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([scale_pos_weight], device=device))\n        optimizer = getattr(optim, config['optimizer'])(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n        for _ in range(20):\n            model.train()\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n        model.eval()\n        preds_list = []\n        with torch.no_grad():\n            for inputs, _ in val_loader:\n                outputs = model(inputs.to(device))\n                preds_list.extend(torch.sigmoid(outputs).cpu().numpy())\n        accuracies.append(accuracy_score(y_val, np.round(preds_list)))\n    tune.report({\"accuracy\": np.mean(accuracies)})\n\ndef main():\n    \"\"\"Main function to run the entire pipeline.\"\"\"\n    if not check_gpu_with_pytorch() or not check_gpu_with_xgboost():\n        return\n\n    print(\"--- Loading Data ---\")\n    train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e7/train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e7/test.csv\")\n\n    # --- Initial Data Cleaning and EDA ---\n    train_df = drop_duplicates(train_df)\n    plot_individual_features(train_df)\n    perform_eda(train_df, stage=\"Initial\")\n    \n    # --- Clean data using baseline model ---\n    cleaned_train_df = analyze_and_clean_with_baseline(train_df)\n    \n    # --- Re-run EDA on cleaned data ---\n    perform_eda(cleaned_train_df, stage=\"Cleaned\")\n    \n    # --- Create and run the preprocessing and feature engineering pipeline ---\n    print(\"\\n--- Starting Preprocessing and Feature Engineering Pipeline ---\")\n    preprocessor = Pipeline(steps=[\n        ('preprocess', FunctionTransformer(preprocess_data)),\n        ('feature_engineer', FunctionTransformer(feature_engineer))\n    ])\n    \n    processed_train_df = preprocessor.fit_transform(cleaned_train_df)\n    processed_test_df = preprocessor.transform(test_df)\n    \n    cluster_features = [\n        'Time_spent_Alone', 'Friends_circle_size', 'Post_frequency',\n        'Social_Interaction_Score', 'Alone_vs_Social_Ratio'\n    ]\n    processed_train_df, processed_test_df = add_cluster_features(processed_train_df, processed_test_df, cluster_features)\n    \n    processed_train_df, processed_test_df = add_groupby_features(processed_train_df, processed_test_df)\n    \n    le = LabelEncoder()\n    le.fit(processed_train_df['Personality'])\n    X = processed_train_df.drop(['id', 'Personality'], axis=1)\n    y = pd.Series(le.transform(processed_train_df['Personality']), index=X.index)\n    X_test = processed_test_df.drop('id', axis=1)\n    X_test = X_test[X.columns]\n\n    scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n    ray.init(num_gpus=torch.cuda.device_count(), ignore_reinit_error=True)\n\n    xgb_search_space = {\"eta\": tune.loguniform(1e-4, 1e-1), \"max_depth\": tune.randint(3, 10)}\n    lgb_search_space = {\"learning_rate\": tune.loguniform(1e-4, 1e-1), \"num_leaves\": tune.randint(20, 100)}\n    nn_search_space = {\"n_layers\": tune.randint(1, 4), \"n_nodes\": tune.choice([32, 64, 128]), \"lr\": tune.loguniform(1e-4, 1e-2), \"dropout_rate\": tune.uniform(0.1, 0.5), \"batch_size\": tune.choice([32, 64]), \"optimizer\": tune.choice([\"Adam\", \"AdamW\"]), \"activation\": tune.choice([\"ReLU\", \"GELU\"]), \"weight_decay\": tune.loguniform(1e-6, 1e-2)}\n\n    def tune_model(train_fn, search_space, name):\n        print(f\"\\n--- Tuning {name} ---\")\n        tuner = Tuner(tune.with_resources(tune.with_parameters(train_fn, data=(X, y, scale_pos_weight)), {\"cpu\": 2, \"gpu\": 1}), param_space=search_space, tune_config=TuneConfig(search_alg=OptunaSearch(metric=\"accuracy\", mode=\"max\"), num_samples=30), run_config=air.RunConfig(name=f\"{name}_tune\", verbose=1))\n        results = tuner.fit()\n        return results.get_best_result(metric=\"accuracy\", mode=\"max\").config\n\n    best_xgb_config = tune_model(train_xgb_cv, xgb_search_space, \"XGBoost\")\n    best_lgb_config = tune_model(train_lgb_cv, lgb_search_space, \"LightGBM\")\n    best_nn_config = tune_model(train_nn_cv, nn_search_space, \"PyTorch_NN\")\n\n    # --- Generate OOF and Test Predictions from Base Models ---\n    oof_xgb, test_preds_xgb = generate_xgb_oof(best_xgb_config, X, y, X_test)\n    oof_lgb, test_preds_lgb = generate_lgb_oof(best_lgb_config, X, y, X_test)\n    oof_nn, test_preds_nn = generate_nn_oof(best_nn_config, X, y, X_test)\n\n    # --- Create Meta-Features ---\n    print(\"\\n--- Creating Meta-Features for Stacking ---\")\n    X_meta = X.copy()\n    X_test_meta = X_test.copy()\n    X_meta['oof_xgb'] = oof_xgb\n    X_meta['oof_lgb'] = oof_lgb\n    X_meta['oof_nn'] = oof_nn\n    X_test_meta['oof_xgb'] = test_preds_xgb\n    X_test_meta['oof_lgb'] = test_preds_lgb\n    X_test_meta['oof_nn'] = test_preds_nn\n\n    # --- Train Meta-Model ---\n    print(\"\\n--- Training Meta-Model (Logistic Regression) ---\")\n    meta_model = LogisticRegression(random_state=42)\n    meta_model.fit(X_meta, y)\n\n    # --- Final Prediction with Meta-Model ---\n    print(\"\\n--- Generating Final Predictions ---\")\n    final_preds = meta_model.predict(X_test_meta)\n    \n    submission_df = pd.DataFrame({'id': test_df['id'], 'Personality': le.inverse_transform(final_preds)})\n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission file created successfully!\")\n\n    ray.shutdown()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"07a5a9c5-71ee-4ecb-a7bc-8abc0ba2cd2e","_cell_guid":"b44f263e-855e-4677-91e0-f2e1a2eb382e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-07-24T18:49:07.866951Z","iopub.execute_input":"2025-07-24T18:49:07.867274Z","iopub.status.idle":"2025-07-24T19:17:42.92403Z","shell.execute_reply.started":"2025-07-24T18:49:07.86725Z","shell.execute_reply":"2025-07-24T19:17:42.923357Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}