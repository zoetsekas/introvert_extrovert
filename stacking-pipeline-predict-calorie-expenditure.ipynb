{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ensemble Model Training Pipeline for Calorie Prediction\n\nThis notebook walks through a complete machine learning pipeline to predict calories burned. The pipeline includes:\n\n1.  **Exploratory Data Analysis (EDA):** Initial analysis and visualization of the data, including a baseline model.\n2.  **Data Cleaning:** Removing duplicates and potentially mislabeled samples identified by the baseline model.\n3.  **Feature Engineering:** Creation of new features (interactions, bins, clusters, and groupby stats) to improve model performance.\n4.  **Feature Selection:** Removing highly correlated features and selecting the top K-best features.\n5.  **Independent Model Tuning:** Using Ray Tune and Optuna to find the best hyperparameters for three separate models (XGBoost, LightGBM, and a PyTorch Neural Network).\n6.  **Stacking Ensemble:** Generating out-of-fold (OOF) predictions from the tuned base models to use as meta-features.\n7.  **Meta-Model Training & Prediction:** Training a final, simpler model on the original and meta-features to generate the final predictions.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, KFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay, auc\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, f_regression\nimport xgboost as xgb\nimport lightgbm as lgb\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport ray\nfrom ray import tune, air\nfrom ray.tune import TuneConfig\nfrom ray.tune.tuner import Tuner\nfrom ray.tune.search.optuna import OptunaSearch\nfrom optuna.samplers import TPESampler\nfrom ray.tune.logger import TBXLoggerCallback\nimport os\nfrom itertools import combinations\nfrom tqdm.auto import tqdm\n\n!pip install tensorboardX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:12:05.633579Z","iopub.execute_input":"2025-07-25T16:12:05.633877Z","iopub.status.idle":"2025-07-25T16:12:08.888360Z","shell.execute_reply.started":"2025-07-25T16:12:05.633857Z","shell.execute_reply":"2025-07-25T16:12:08.887352Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 1. Environment Setup and GPU Checks","metadata":{}},{"cell_type":"code","source":"# --- Environment Variable to Suppress Warnings ---\nos.environ['RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS'] = '0'\n\ndef check_gpu_with_pytorch():\n    \"\"\"Checks if PyTorch can access and use the GPU, printing the status.\"\"\"\n    print(\"Performing GPU check for PyTorch...\")\n    if torch.cuda.is_available():\n        print(f\"✅ GPU check successful. PyTorch can access GPU: {torch.cuda.get_device_name(0)}\")\n        return True\n    else:\n        print(\"❌ GPU check FAILED. PyTorch could not find a CUDA-enabled GPU.\")\n        return False\n\ndef check_gpu_with_xgboost():\n    \"\"\"Checks if XGBoost can access and use the GPU, printing the status.\"\"\"\n    print(\"Performing GPU check for XGBoost...\")\n    try:\n        xgb.train({'device': 'cuda'}, xgb.DMatrix(np.random.rand(10, 2), label=np.random.rand(10)), 1)\n        print(\"✅ GPU check successful. XGBoost can access the GPU.\")\n        return True\n    except xgb.core.XGBoostError:\n        print(\"❌ GPU check FAILED. XGBoost could not utilize the GPU.\")\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:12:08.890135Z","iopub.execute_input":"2025-07-25T16:12:08.890419Z","iopub.status.idle":"2025-07-25T16:12:08.899296Z","shell.execute_reply.started":"2025-07-25T16:12:08.890392Z","shell.execute_reply":"2025-07-25T16:12:08.898515Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## 2. PyTorch Neural Network and Custom Loss Definition","metadata":{}},{"cell_type":"code","source":"class RMSLELoss(nn.Module):\n    \"\"\"Custom Root Mean Squared Logarithmic Error loss function.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self, pred, actual):\n        # Add 1 to prevent log(0)\n        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n\nclass Net(nn.Module):\n    \"\"\"A dynamically configurable multi-layer perceptron for regression.\"\"\"\n    def __init__(self, input_dim, n_layers, n_nodes, dropout_rate, activation_fn):\n        \"\"\"\n        Initializes the neural network.\n\n        Args:\n            input_dim (int): The number of input features.\n            n_layers (int): The number of hidden layers.\n            n_nodes (int): The number of nodes in each hidden layer.\n            dropout_rate (float): The dropout rate to apply after each hidden layer.\n            activation_fn (str): The name of the torch.nn activation function to use.\n        \"\"\"\n        super(Net, self).__init__()\n        layers = []\n        act_fn = getattr(nn, activation_fn)()\n        layers.append(nn.Linear(input_dim, n_nodes))\n        layers.append(act_fn)\n        layers.append(nn.Dropout(dropout_rate))\n        for _ in range(n_layers - 1):\n            layers.append(nn.Linear(n_nodes, n_nodes))\n            layers.append(act_fn)\n            layers.append(nn.Dropout(dropout_rate))\n        layers.append(nn.Linear(n_nodes, 1))\n        # Add a final ReLU to ensure non-negative calorie predictions\n        layers.append(nn.ReLU())\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"Defines the forward pass of the neural network.\"\"\"\n        return self.network(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:12:08.900028Z","iopub.execute_input":"2025-07-25T16:12:08.900217Z","iopub.status.idle":"2025-07-25T16:12:08.922574Z","shell.execute_reply.started":"2025-07-25T16:12:08.900201Z","shell.execute_reply":"2025-07-25T16:12:08.921778Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## 3. Data Processing, EDA, and Feature Engineering Functions","metadata":{}},{"cell_type":"code","source":"def perform_eda(df, stage=\"Initial\"):\n    \"\"\"\n    Performs and prints basic Exploratory Data Analysis on the dataframe.\n\n    Args:\n        df (pd.DataFrame): The dataframe to analyze.\n        stage (str): A string to identify the stage of EDA (e.g., \"Initial\", \"Cleaned\").\n    \"\"\"\n    print(f\"\\n--- {stage} Exploratory Data Analysis ---\")\n    print(\"\\nDataset Info:\")\n    df.info()\n    print(\"\\nDescriptive Statistics:\")\n    print(df.describe())\n    \n    # Visualize the correlation matrix of the original features\n    print(\"\\nCorrelation Matrix:\")\n    eda_df = df.copy()\n    for col in eda_df.columns:\n        if eda_df[col].dtype == 'object':\n            eda_df[col] = LabelEncoder().fit_transform(eda_df[col])\n    corr_matrix = eda_df.drop('id', axis=1).corr()\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.title(f'{stage} Correlation Matrix of Features')\n    plt.savefig(f'{stage.lower()}_correlation_matrix.png')\n    print(f\"{stage} correlation matrix plot saved to {stage.lower()}_correlation_matrix.png\")\n    plt.show()\n\ndef analyze_and_clean_with_baseline(df):\n    \"\"\"\n    Trains a baseline model to find and remove potentially mislabeled data.\n\n    Args:\n        df (pd.DataFrame): The training dataframe to clean.\n\n    Returns:\n        pd.DataFrame: The cleaned dataframe with suspicious labels removed.\n    \"\"\"\n    print(\"\\n--- Baseline Model Analysis and Cleaning ---\")\n    baseline_df = preprocess_data(df.copy())\n    X = baseline_df.drop(['id', 'Calories'], axis=1)\n    y = baseline_df['Calories']\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    baseline_model = LinearRegression()\n    baseline_model.fit(X_scaled, y)\n    y_hat = baseline_model.predict(X_scaled)\n\n    # Identify samples with large residuals (errors)\n    residuals = np.abs(y - y_hat)\n    # Exclude samples where the error is in the top 1%\n    exclude_indices = df.index[residuals > np.percentile(residuals, 99)]\n    \n    print(f\"Identified {len(exclude_indices)} samples to remove based on large baseline model residuals.\")\n    \n    cleaned_df = df.drop(exclude_indices).reset_index(drop=True)\n    print(f\"Original shape: {df.shape}, Cleaned shape: {cleaned_df.shape}\")\n    \n    return cleaned_df\n\ndef drop_duplicates(df):\n    \"\"\"Checks for and removes duplicate rows from the dataframe.\"\"\"\n    print(\"\\n--- Dropping Duplicates ---\")\n    print(f\"Original shape: {df.shape}\")\n    df_cleaned = df.drop_duplicates()\n    print(f\"Shape after dropping duplicates: {df_cleaned.shape}\")\n    print(f\"Removed {df.shape[0] - df_cleaned.shape[0]} duplicate rows.\")\n    return df_cleaned\n\ndef plot_individual_features(df):\n    \"\"\"Creates and saves a pairplot of numerical features.\"\"\"\n    print(\"\\n--- Creating Pairplot of Numerical Features ---\")\n    numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n    if 'id' in numerical_features:\n        numerical_features.remove('id')\n    \n    # Create a dataframe for the pairplot\n    pairplot_df = df[numerical_features]\n    \n    sns.pairplot(pairplot_df, diag_kind='kde')\n    plt.suptitle('Pairplot of Numerical Features', y=1.02)\n    plt.savefig('eda_pairplot.png')\n    print(\"Pairplot saved to eda_pairplot.png\")\n    plt.show()\n\ndef preprocess_data(df):\n    \"\"\"Preprocesses the data by encoding categorical features.\"\"\"\n    df_copy = df.copy()\n    if 'Sex' in df_copy.columns and df_copy['Sex'].dtype == 'object':\n        df_copy['Sex'] = LabelEncoder().fit_transform(df_copy['Sex'])\n    return df_copy\n\ndef feature_engineer(df):\n    \"\"\"Creates new interaction, ratio, and binned features.\"\"\"\n    df_copy = df.copy()\n    # BMI calculation (Height in meters)\n    df_copy['BMI'] = df_copy['Weight'] / ((df_copy['Height'] / 100) ** 2)\n    df_copy['Duration_x_HeartRate'] = df_copy['Duration'] * df_copy['Heart_Rate']\n    df_copy['Temp_x_HeartRate'] = df_copy['Body_Temp'] * df_copy['Heart_Rate']\n    \n    # Binning Features\n    df_copy['Age_Bin'] = pd.qcut(df_copy['Age'], q=5, labels=False, duplicates='drop')\n    df_copy['BMI_Bin'] = pd.qcut(df_copy['BMI'], q=5, labels=False, duplicates='drop')\n    df_copy['Heart_Rate_Bin'] = pd.qcut(df_copy['Heart_Rate'], q=5, labels=False, duplicates='drop')\n    return df_copy\n\ndef add_cluster_features(train_df, test_df, features, n_clusters=4):\n    \"\"\"Adds a new feature based on K-Means clustering.\"\"\"\n    print(f\"\\nCreating user clusters with K-Means (k={n_clusters})...\")\n    scaler = StandardScaler()\n    train_scaled = scaler.fit_transform(train_df[features])\n    test_scaled = scaler.transform(test_df[features])\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    train_df['User_Cluster'] = kmeans.fit_predict(train_scaled)\n    test_df['User_Cluster'] = kmeans.predict(test_scaled)\n    return train_df, test_df\n\ndef add_groupby_features(train_df, test_df):\n    \"\"\"Creates groupby aggregate features.\"\"\"\n    print(\"\\nAdding groupby aggregate features...\")\n    \n    groupby_cols = ['Age_Bin', 'BMI_Bin', 'Heart_Rate_Bin', 'Sex', 'User_Cluster']\n    agg_cols = ['Duration', 'Heart_Rate', 'Body_Temp', 'BMI']\n    \n    for group_col in tqdm(groupby_cols, desc=\"Groupby Features\"):\n        agg_stats = train_df.groupby(group_col)[agg_cols].agg(['mean', 'std', 'min', 'max']).reset_index()\n        \n        # Create unique column names to avoid collisions\n        new_cols = [group_col]\n        for col in agg_stats.columns.levels[0]:\n            if col == group_col:\n                continue\n            for stat in agg_stats.columns.levels[1]:\n                if stat == '':\n                    continue\n                new_cols.append(f'{group_col}_{col}_{stat}')\n        agg_stats.columns = new_cols\n        \n        train_df = pd.merge(train_df, agg_stats, on=group_col, how='left')\n        test_df = pd.merge(test_df, agg_stats, on=group_col, how='left')\n        \n        # Fill NaNs in test set after the merge\n        for col in new_cols:\n            if col != group_col:\n                median_val = train_df[col].median()\n                test_df[col] = test_df[col].fillna(median_val)\n\n    return train_df, test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:12:08.924499Z","iopub.execute_input":"2025-07-25T16:12:08.924781Z","iopub.status.idle":"2025-07-25T16:12:08.946134Z","shell.execute_reply.started":"2025-07-25T16:12:08.924764Z","shell.execute_reply":"2025-07-25T16:12:08.945357Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## 4. Ray Tune Training Functions","metadata":{}},{"cell_type":"code","source":"def rmsle(y_true, y_pred):\n    \"\"\"Custom Root Mean Squared Logarithmic Error metric.\"\"\"\n    return np.sqrt(mean_squared_log_error(y_true, np.maximum(0, y_pred)))\n\ndef train_xgb_cv(config, data):\n    \"\"\"Ray Tune trainable function for XGBoost CV.\"\"\"\n    X, y = data\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    rmsles = []\n    for train_idx, val_idx in kf.split(X):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        params = {'objective': 'reg:squarederror', 'eval_metric': 'rmsle', 'tree_method': 'hist', 'device': 'cuda', **config}\n        model = xgb.train(params, dtrain, 1000, evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=False)\n        preds = model.predict(dval, iteration_range=(0, model.best_iteration))\n        rmsles.append(rmsle(y_val, preds))\n    tune.report(rmsle=np.mean(rmsles))\n\ndef train_lgb_cv(config, data):\n    \"\"\"Ray Tune trainable function for LightGBM CV.\"\"\"\n    X, y = data\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    rmsles = []\n    for train_idx, val_idx in kf.split(X):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n        params = {'objective': 'regression_l1', 'metric': 'rmsle', 'device': 'gpu', 'verbose': -1, **config}\n        model = lgb.train(params, dtrain, 1000, valid_sets=[dval], callbacks=[lgb.early_stopping(50, verbose=False)])\n        preds = model.predict(X_val, num_iteration=model.best_iteration)\n        rmsles.append(rmsle(y_val, preds))\n    tune.report(rmsle=np.mean(rmsles))\n\ndef train_nn_cv(config, data):\n    \"\"\"Ray Tune trainable function for PyTorch NN CV.\"\"\"\n    X, y = data\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    rmsles = []\n    for train_idx, val_idx in kf.split(X):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        scaler = StandardScaler()\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_val_scaled = scaler.transform(X_val)\n        train_ds = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1))\n        val_ds = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32), torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1))\n        train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n        val_loader = DataLoader(val_ds, batch_size=config[\"batch_size\"])\n        model = Net(X.shape[1], config['n_layers'], config['n_nodes'], config['dropout_rate'], config['activation']).to(device)\n        criterion = RMSLELoss()\n        optimizer = getattr(optim, config['optimizer'])(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n        for _ in range(25):\n            model.train()\n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n        model.eval()\n        preds_list = []\n        with torch.no_grad():\n            for inputs, _ in val_loader:\n                outputs = model(inputs.to(device))\n                preds_list.extend(outputs.cpu().numpy())\n        rmsles.append(rmsle(y_val, np.array(preds_list).flatten()))\n    tune.report(rmsle=np.mean(rmsles))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:12:08.947008Z","iopub.execute_input":"2025-07-25T16:12:08.947225Z","iopub.status.idle":"2025-07-25T16:12:08.977140Z","shell.execute_reply.started":"2025-07-25T16:12:08.947200Z","shell.execute_reply":"2025-07-25T16:12:08.976327Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## 5. Main Execution Block","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Main function to run the entire pipeline.\"\"\"\n    if not check_gpu_with_pytorch() or not check_gpu_with_xgboost():\n        return\n\n    print(\"--- Loading Data ---\")\n    train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e5/train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e5/test.csv\")\n\n    # --- Initial Data Cleaning and EDA ---\n    train_df = drop_duplicates(train_df)\n    plot_individual_features(train_df)\n    perform_eda(train_df, stage=\"Initial\")\n    \n    # --- Clean data using baseline model ---\n    cleaned_train_df = analyze_and_clean_with_baseline(train_df)\n    \n    # --- Re-run EDA on cleaned data ---\n    perform_eda(cleaned_train_df, stage=\"Cleaned\")\n    \n    # --- Create and run the preprocessing and feature engineering pipeline ---\n    print(\"\\n--- Starting Preprocessing and Feature Engineering Pipeline ---\")\n    preprocessor = Pipeline(steps=[\n        ('preprocess', FunctionTransformer(preprocess_data)),\n        ('feature_engineer', FunctionTransformer(feature_engineer))\n    ])\n    \n    processed_train_df = preprocessor.fit_transform(cleaned_train_df)\n    processed_test_df = preprocessor.transform(test_df)\n    \n    cluster_features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'BMI']\n    processed_train_df, processed_test_df = add_cluster_features(processed_train_df, processed_test_df, cluster_features)\n    \n    processed_train_df, processed_test_df = add_groupby_features(processed_train_df, processed_test_df)\n    \n    X = processed_train_df.drop(['id', 'Calories'], axis=1)\n    y = processed_train_df['Calories']\n    X_test = processed_test_df.drop('id', axis=1)\n    X_test = X_test[X.columns]\n\n    ray.init(num_gpus=torch.cuda.device_count(), ignore_reinit_error=True)\n\n    # --- Define Expanded Independent Search Spaces ---\n    xgb_search_space = {\n        \"eta\": tune.loguniform(1e-4, 1e-1), \"max_depth\": tune.randint(3, 12), \"subsample\": tune.uniform(0.5, 1.0),\n        \"colsample_bytree\": tune.uniform(0.5, 1.0), \"min_child_weight\": tune.quniform(1, 20, 1), \"gamma\": tune.uniform(0, 0.7),\n        \"reg_alpha\": tune.loguniform(1e-8, 1.0), \"reg_lambda\": tune.loguniform(1e-8, 1.0)\n    }\n    lgb_search_space = {\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1), \"num_leaves\": tune.randint(20, 150), \"subsample\": tune.uniform(0.5, 1.0),\n        \"colsample_bytree\": tune.uniform(0.5, 1.0), \"min_child_samples\": tune.randint(5, 50), \"reg_alpha\": tune.loguniform(1e-8, 1.0),\n        \"reg_lambda\": tune.loguniform(1e-8, 1.0)\n    }\n    nn_search_space = {\n        \"n_layers\": tune.randint(1, 4), \"n_nodes\": tune.choice([32, 64, 128, 256]), \"lr\": tune.loguniform(1e-4, 1e-2),\n        \"dropout_rate\": tune.uniform(0.1, 0.5), \"batch_size\": tune.choice([32, 64, 128]), \"optimizer\": tune.choice([\"Adam\", \"AdamW\", \"RMSprop\"]),\n        \"activation\": tune.choice([\"ReLU\", \"LeakyReLU\", \"GELU\"]), \"weight_decay\": tune.loguniform(1e-6, 1e-2)\n    }\n\n    # --- Tune Each Model Independently ---\n    def tune_model(train_fn, search_space, name):\n        print(f\"\\n--- Tuning {name} ---\")\n        \n        run_config = air.RunConfig(\n            name=f\"{name}_tune\",\n            verbose=1,\n            storage_path=\"/kaggle/working/ray_results\",\n            callbacks=[TBXLoggerCallback()]\n        )\n        \n        tuner = Tuner(\n            tune.with_resources(tune.with_parameters(train_fn, data=(X, y)), {\"cpu\": 2, \"gpu\": 1}),\n            param_space=search_space,\n            tune_config=TuneConfig(search_alg=OptunaSearch(metric=\"rmsle\", mode=\"min\"), num_samples=30),\n            run_config=run_config\n        )\n        results = tuner.fit()\n        best_config = results.get_best_result(metric=\"rmsle\", mode=\"min\").config\n        print(f\"Best config for {name}: {best_config}\")\n        return best_config\n\n    best_xgb_config = tune_model(train_xgb_cv, xgb_search_space, \"XGBoost\")\n    best_lgb_config = tune_model(train_lgb_cv, lgb_search_space, \"LightGBM\")\n    best_nn_config = tune_model(train_nn_cv, nn_search_space, \"PyTorch_NN\")\n\n    # --- Train Final Models with Best Hyperparameters ---\n    print(\"\\n--- Training final XGBoost model ---\")\n    final_xgb_model = xgb.train({'objective': 'reg:squarederror', 'tree_method': 'hist', 'device': 'cuda', **best_xgb_config}, xgb.DMatrix(X, label=y), 1200)\n    \n    print(\"\\n--- Training final LightGBM model ---\")\n    final_lgb_model = lgb.train({'objective': 'regression_l1', 'device': 'gpu', 'verbose': -1, **best_lgb_config}, lgb.Dataset(X, label=y), 1200)\n\n    print(\"\\n--- Training final PyTorch NN model ---\")\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    final_nn_model = Net(X.shape[1], best_nn_config['n_layers'], best_nn_config['n_nodes'], best_nn_config['dropout_rate'], best_nn_config['activation']).to(device)\n    optimizer_class = getattr(optim, best_nn_config['optimizer'])\n    optimizer = optimizer_class(final_nn_model.parameters(), lr=best_nn_config['lr'], weight_decay=best_nn_config['weight_decay'])\n    train_ds = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32), torch.tensor(y.values, dtype=torch.float32).unsqueeze(1))\n    train_loader = DataLoader(train_ds, batch_size=best_nn_config['batch_size'], shuffle=True)\n    criterion = RMSLELoss()\n    for epoch in tqdm(range(50), desc=\"Final NN Training\"): # More epochs for final training\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = final_nn_model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n    # --- Final Ensemble Prediction ---\n    print(\"\\n--- Generating Final Predictions ---\")\n    final_xgb_preds = final_xgb_model.predict(xgb.DMatrix(X_test))\n    final_lgb_preds = final_lgb_model.predict(X_test)\n    final_nn_model.eval()\n    X_test_scaled = scaler.transform(X_test)\n    with torch.no_grad():\n        final_nn_preds = final_nn_model(torch.tensor(X_test_scaled, dtype=torch.float32).to(device)).cpu().numpy().flatten()\n\n    final_ensemble_preds = (final_xgb_preds + final_lgb_preds + final_nn_preds) / 3\n    \n    submission_df = pd.DataFrame({'id': test_df['id'], 'Calories': final_ensemble_preds})\n    submission_df.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission file created successfully!\")\n\n    ray.shutdown()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:12:08.978146Z","iopub.execute_input":"2025-07-25T16:12:08.978429Z","iopub.status.idle":"2025-07-25T16:12:08.997886Z","shell.execute_reply.started":"2025-07-25T16:12:08.978404Z","shell.execute_reply":"2025-07-25T16:12:08.997120Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## 6. Run the Pipeline","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:12:08.998672Z","iopub.execute_input":"2025-07-25T16:12:08.998915Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div class=\"tuneStatus\">\n  <div style=\"display: flex;flex-direction: row\">\n    <div style=\"display: flex;flex-direction: column;\">\n      <h3>Tune Status</h3>\n      <table>\n<tbody>\n<tr><td>Current time:</td><td>2025-07-25 16:19:34</td></tr>\n<tr><td>Running for: </td><td>00:04:16.37        </td></tr>\n<tr><td>Memory:      </td><td>11.3/31.4 GiB      </td></tr>\n</tbody>\n</table>\n    </div>\n    <div class=\"vDivider\"></div>\n    <div class=\"systemInfo\">\n      <h3>System Info</h3>\n      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/4 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:P100)\n    </div>\n    \n  </div>\n  <div class=\"hDivider\"></div>\n  <div class=\"trialStatus\">\n    <h3>Trial Status</h3>\n    <table>\n<thead>\n<tr><th>Trial name           </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  colsample_bytree</th><th style=\"text-align: right;\">     eta</th><th style=\"text-align: right;\">   gamma</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  min_child_weight</th><th style=\"text-align: right;\">  reg_alpha</th><th style=\"text-align: right;\">  reg_lambda</th><th style=\"text-align: right;\">  subsample</th></tr>\n</thead>\n<tbody>\n<tr><td>train_xgb_cv_3e59c973</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">          0.908568</td><td style=\"text-align: right;\">0.019935</td><td style=\"text-align: right;\">0.384899</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">                11</td><td style=\"text-align: right;\">  0.0597854</td><td style=\"text-align: right;\"> 1.54407e-07</td><td style=\"text-align: right;\">   0.969524</td></tr>\n</tbody>\n</table>\n  </div>\n</div>\n<style>\n.tuneStatus {\n  color: var(--jp-ui-font-color1);\n}\n.tuneStatus .systemInfo {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus td {\n  white-space: nowrap;\n}\n.tuneStatus .trialStatus {\n  display: flex;\n  flex-direction: column;\n}\n.tuneStatus h3 {\n  font-weight: bold;\n}\n.tuneStatus .hDivider {\n  border-bottom-width: var(--jp-border-width);\n  border-bottom-color: var(--jp-border-color0);\n  border-bottom-style: solid;\n}\n.tuneStatus .vDivider {\n  border-left-width: var(--jp-border-width);\n  border-left-color: var(--jp-border-color0);\n  border-left-style: solid;\n  margin: 0.5em 1em 0.5em 1em;\n}\n</style>\n"},"metadata":{}}],"execution_count":null}]}